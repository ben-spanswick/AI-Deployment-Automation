# AI Box Docker Compose Configuration - Modern Stack
# Generated by Ansible deployment

services:
  # LocalAI - OpenAI-compatible local LLM API
  localai:
    image: quay.io/go-skynet/local-ai:latest-gpu-nvidia-cuda-12
    container_name: localai
    ports:
      - "{{ localai_port }}:8080"
    volumes:
      - {{ ai_box_dir }}/models:/build/models
      - localai-data:/tmp/generated
    environment:
      - THREADS={{ localai_threads | default(8) }}
      - DEBUG={{ debug_mode | default('false') }}
      - CUDA_VISIBLE_DEVICES={{ localai_gpus }}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
{% if ',' in localai_gpus %}
              device_ids: ['{{ localai_gpus.split(",") | join("','") }}']
{% else %}
              device_ids: ['{{ localai_gpus }}']
{% endif %}
              capabilities: [gpu]
    restart: unless-stopped
    networks:
      - ai-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/v1/models"]
      interval: 30s
      timeout: 10s
      retries: 3
  
  # Ollama - Easy model management and API
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "{{ ollama_port }}:11434"
    volumes:
      - ollama-data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_DEBUG={{ ollama_debug | default('info') }}
      - CUDA_VISIBLE_DEVICES={{ ollama_gpus }}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
{% if ',' in ollama_gpus %}
              device_ids: ['{{ ollama_gpus.split(",") | join("','") }}']
{% else %}
              device_ids: ['{{ ollama_gpus }}']
{% endif %}
              capabilities: [gpu]
    restart: unless-stopped
    networks:
      - ai-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/"]
      interval: 30s
      timeout: 10s
      retries: 3
      
  # Stable Diffusion WebUI Forge
  forge:
    image: nykk3/stable-diffusion-webui-forge:latest
    container_name: forge
    ports:
      - "{{ forge_port }}:7860"
    volumes:
      # Model directories
      - {{ ai_box_dir }}/models/stable-diffusion:/app/stable-diffusion-webui/models/Stable-diffusion
      - {{ ai_box_dir }}/models/stable-diffusion/SDXL:/app/stable-diffusion-webui/models/SDXL  
      - {{ ai_box_dir }}/models/vae:/app/stable-diffusion-webui/models/VAE
      - {{ ai_box_dir }}/models/loras:/app/stable-diffusion-webui/models/Lora
      - {{ ai_box_dir }}/models/embeddings:/app/stable-diffusion-webui/models/embeddings
      # Output directory
      - {{ ai_box_dir }}/outputs/forge:/app/stable-diffusion-webui/outputs
      # Extensions directory
      - forge-extensions:/app/stable-diffusion-webui/extensions
    environment:
      - COMMANDLINE_ARGS=--listen --port 7860 --api --xformers --enable-insecure-extension-access --skip-torch-cuda-test --skip-version-check --no-download-sd-model {{ forge_extra_args | default('') }}
      - CUDA_VISIBLE_DEVICES={{ forge_gpus }}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
{% if ',' in forge_gpus %}
              device_ids: ['{{ forge_gpus.split(",") | join("','") }}']
{% else %}
              device_ids: ['{{ forge_gpus }}']
{% endif %}
              capabilities: [gpu]
        limits:
          memory: {{ forge_memory_limit | default('24G') }}
    restart: unless-stopped
    shm_size: {{ forge_shm_size | default('12gb') }}
    networks:
      - ai-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7860/"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s

{% if enable_dcgm %}
  # NVIDIA DCGM GPU Metrics
  dcgm-exporter:
    image: nvcr.io/nvidia/k8s/dcgm-exporter:3.1.8-3.1.5-ubuntu20.04
    container_name: dcgm-exporter
    ports:
      - "{{ dcgm_port | default(9400) }}:9400"
    environment:
      - DCGM_EXPORTER_LISTEN=0.0.0.0:9400
      - DCGM_EXPORTER_KUBERNETES=false
      - DCGM_EXPORTER_COLLECTORS=/etc/dcgm-exporter/dcp-metrics-included.csv
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
              count: all
    restart: unless-stopped
    cap_add:
      - SYS_ADMIN
    networks:
      - ai-network
{% endif %}

{% if enable_dashboard %}
  # Nginx Dashboard
  nginx-dashboard:
    image: nginx:alpine
    container_name: nginx-dashboard
    ports:
      - "80:80"
    volumes:
      - {{ ai_box_dir }}/nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - {{ ai_box_dir }}/nginx/html:/usr/share/nginx/html:ro
    restart: unless-stopped
    networks:
      - ai-network
    depends_on:
      - localai
      - ollama
      - forge
{% endif %}

{% if enable_watchtower %}
  # Watchtower for automatic updates
  watchtower:
    image: containrrr/watchtower
    container_name: watchtower
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - WATCHTOWER_CLEANUP=true
      - WATCHTOWER_POLL_INTERVAL=86400  # 24 hours
      - WATCHTOWER_INCLUDE_STOPPED=false
      - WATCHTOWER_MONITOR_ONLY=false
    restart: unless-stopped
    networks:
      - ai-network
{% endif %}

# Network configuration
networks:
  ai-network:
    driver: bridge
    name: ai-network
    ipam:
      config:
        - subnet: 172.20.0.0/16

# Named volumes for persistent data
volumes:
  localai-data:
    driver: local
  ollama-data:
    driver: local
  forge-extensions:
    driver: local
  forge-config: 
    driver: local